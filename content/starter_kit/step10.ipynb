{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Agentは特徴量抽出が大切。\n",
    "\n",
    " Agentとbrain(model)の使い分けてです。\n",
    "\n",
    " modelには、できる限り処理された情報のみを入れたほうが、効率よく学習が進みます。また、actionが存在するかどうかの処理や、actionが有効かどうかの処理などは、やめたほうがいい。また、データを管理して学習を効率よく進める部分なども、やめたほうがいい。そのような、入力と出力のルール化みたいところを、ラップでくるんで行うのがAgentです。\n",
    "\n",
    "action = agent(state)\n",
    "\n",
    "主に3つの機能を有するべきです。\n",
    "* 入力データから特徴量を抽出する機能\n",
    "* 出力データの形式を整える機能\n",
    "* 学習を効率よく進める機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 コードを見ていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import dezero_emb as dezero\n",
    "class DQNAgent:\n",
    "  def __init__(self):\n",
    "    self.epsilon = CFG_ML.initial_epsilon\n",
    "    self.action_size = 2\n",
    "\n",
    "    self.replay_buffer = ReplayBuffer(CFG_ML.buffer_size, CFG_ML.batch_size)\n",
    "    self.qnet = DQNet()\n",
    "    self.qnet_target = DQNet()\n",
    "    self.optimizer = dezero.optimizers.Adam(CFG_ML.lr)\n",
    "    self.optimizer.setup(self.qnet)\n",
    "\n",
    "  def __call__(self, board, puyo):\n",
    "    action_list = utils.create_action_list(board)\n",
    "\n",
    "    next_boards = []\n",
    "    next_reward =[]\n",
    "    action =(2, 1)\n",
    "    if len(action_list):\n",
    "      for action in action_list:\n",
    "        next_board, reward, done = utils.next_board(board, puyo, action)\n",
    "        if not done:\n",
    "          next_boards.append(next_board)\n",
    "          next_reward.append(reward)\n",
    "      \n",
    "      next_boards = np.stack(next_boards)\n",
    "      predictions = self.eval2(next_boards)\n",
    "      \n",
    "      next_reward =np.array(next_reward)[:, np.newaxis]\n",
    "      predictions += dezero.Variable(next_reward)\n",
    "      index = predictions.data.argmax()\n",
    "      action = action_list[index]\n",
    "    return action\n",
    "\n",
    "\n",
    "  def boardtostate(self, board):\n",
    "    cont_b = 2 ** np.arange(CFG.Width,dtype=np.int32)\n",
    "    b1 = np.zeros(CFG.Height * CFG.Width,dtype = np.int32).reshape(CFG.Height , CFG.Width)\n",
    "    b1[board == 1] = 1\n",
    "    b2 = np.zeros(CFG.Height * CFG.Width,dtype = np.int32).reshape(CFG.Height , CFG.Width)\n",
    "    b2[board == 2] = 1\n",
    "    b3 = np.zeros(CFG.Height * CFG.Width,dtype = np.int32).reshape(CFG.Height , CFG.Width)\n",
    "    b3[board == 3] = 1\n",
    "    b4 = np.zeros(CFG.Height * CFG.Width,dtype = np.int32).reshape(CFG.Height , CFG.Width)\n",
    "    b4[board == 4] = 1\n",
    "    board_list = np.concatenate([b1,b2,b3,b4])\n",
    "    state =  board_list.dot(cont_b)      \n",
    "    return state\n",
    "\n",
    "  def eval(self, board):\n",
    "    state = self.boardtostate(board)      \n",
    "    return self.qnet_target(state)\n",
    "\n",
    "  def eval2(self, boards):\n",
    "    states = []\n",
    "    for i in range(boards.shape[0]):\n",
    "      state = self.boardtostate(boards[i])\n",
    "      states.append(state)\n",
    "    states = np.stack(states)      \n",
    "    return self.qnet_target(states)\n",
    "\n",
    "\n",
    "  def update(self, board, action, reward, next_board, done):\n",
    "    state =  self.boardtostate(board) \n",
    "    next_state =  self.boardtostate(next_board)      \n",
    "    \n",
    "    self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "    if not done:\n",
    "      return\n",
    "    if len(self.replay_buffer) < CFG_ML.batch_size:\n",
    "      return\n",
    "    state, action, reward, next_state, done = self.replay_buffer.get_batch()\n",
    "\n",
    "    qs = self.qnet(state)\n",
    "    next_qs = self.qnet_target(next_state)\n",
    "    reward =reward[:,np.newaxis]\n",
    "    done =done[:,np.newaxis]\n",
    "    target = reward + (1 - done) * CFG_ML.gamma * next_qs\n",
    "\n",
    "    self.qnet.cleargrads()\n",
    "    loss = dezero.F.mean_squared_error(qs, target)\n",
    "    loss.backward()\n",
    "    self.optimizer.update()\n",
    "\n",
    "\n",
    "  def sync_qnet(self):\n",
    "    self.qnet_target = copy.deepcopy(self.qnet)\n",
    "\n",
    "  def save_model(self,filename):\n",
    "    self.qnet.save_weights(filename)\n",
    "\n",
    "  def load_model(self,filename):\n",
    "    self.qnet.load_weights(filename)\n",
    "    self.qnet_target.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 init\n",
    "\n",
    "初期化には、agentが持つデータが定義されます。４つあります。\n",
    "* replay_buffer\n",
    "* DQNet : 訓練対象\n",
    "* target_DQNet : 評価用\n",
    "* optimizer : lossは関数を使うので、optimizerだけです。\n",
    "２つのmodelの使い分けですが、評価用のmodelで計算したpredictionsを使って、訓練用のmodelを学習させます。訓練後に、sync_qnetで同期します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 board to state\n",
    "\n",
    "* cont_bは、boardの横１行のデータを２進法でビット化するために使います。これで、12行6列の72個のデータを、12個に圧縮して特徴量にします。4色あるので、288個のデータが、48個になります。\n",
    "* np.concatenate([b1,b2,b3,b4]) : 4色の2次元データを連結しています。\n",
    "* board_list.dot(cont_b) : board_list と cont_b の行列積を計算します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3  call\n",
    "agent()のように呼び出されたときに、この関数が動きます。\n",
    "* action_list　から　next_board　を計算して　next_statesを作ります。fallblockは、blockを落とす場所をすれば、必ず決まった挙動をするゲームなので、計算可能です。計算ができないゲームでは、この手法は使えません。\n",
    "* next_statesから、Qを推測します(predictions)。predictionsにrewardを加えて、一番大きいものを選択します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 update\n",
    "\n",
    "* データをreplay bufferに入れていきます。\n",
    "* done（終了）で、バッチデータから学習をします。\n",
    "* qnet.cleargrads : 累積しているgradを０にします。\n",
    "* loss.backward : 逆伝播させます。\n",
    "* optimizer.update : パラメータに反映させます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
