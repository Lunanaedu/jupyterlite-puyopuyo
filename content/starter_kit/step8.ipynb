{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8 basic DQN　& brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 DQNの基礎\n",
    "\n",
    "深層強化学習の基本である、DQNの基礎から始めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 人工知能と機械学習の基本\n",
    "\n",
    "　人工知能は、ある状態から、何かを推測します。例えば、写真から、映っているのが犬だと推測します。この推測をevaluationあるいは、predictionと英語で書くことが多く、プログラムにもよく見られます。evaluationは、value（価値）をつけるという言葉なので、ある状態から推測するということは、何かの価値をつけるということでもあります。\n",
    "\n",
    "　人工知能の推測できる正確性は、良くても８０％くらいです。複数の可能性を同時に出すことが多くて、例えば、写真に写っているものの可能性が、イヌ８０％、ネコ５０％、ウマ１０％のように推測します。そこで、一番高い可能性のものだけを表示していることがよくあります。\n",
    "\n",
    "　人工知能の一部には、推測ではなくて決定だけをするものがあります。それは、あらかじめ決められた計算式で答えを出すものです。例えば、２つの数値が与えられた場合に、必ず合計を出すものです。これが人工知能と呼べるかどうかは、疑問ではありますが、強化学習においてはheuristic、経験則などと呼ばれています。\n",
    "\n",
    "　heuristicが人工知能と呼べるか疑問なのは、学習が存在しないからです。経験値であるデータが積み上がり、それを統計処理して経験則を算出して、計算式を更新することはできますが、これを学習というかは微妙です。\n",
    "\n",
    "　機械学習は、人工知能が学習することです。大量のデータを統計処理して、経験則を出すことは、極めて高度な数学を要するかもしれません。それを、「だいたい正しい」方法にするのが、人工知能の役割です。この「だいたい正しい」という部分が、とても大切です。\n",
    "\n",
    "　例えば、優勝チームを推測することを考えてみます。まだ試合をしていないのだから、何が起きるかは分かりません。しかし、それまでの経験から、守備力が強いチームが勝つとか、攻撃力の強いチームが勝つとか、いろいろな特徴から推測はできそうです。この特徴を数値化して、過去の環境を再現して、勝敗をつけることができれば、良さそうです。\n",
    "\n",
    "　このように、機械学習の基本は、過去のデータから、特徴量を抽出して、その環境を再現して、結果を出して、訓練することです。画像系の機械学習では、環境再現部分がなくて、過去のデータと結果だけで訓練することが多く、ゲームなどの強化学習では、過去のデータがほとんどありません。自然言語系は、その中間です。\n",
    "\n",
    "　自然言語系が、環境の再現をしているかについては、chatGPTなどの現在の商用AIは、内容を公開していないので分かりません。過去のtransformerは学習に使用していました。文を構成する単語間の結びつきを「人が文を理解する環境」として再現して、学習を進めています。\n",
    "\n",
    "　機械学習は英語で、machine learning　でありMLと略されます。学習はlearningですが、trainが良く使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 brain model\n",
    "\n",
    "　人工知能の脳モデルを、brainあるいはbrain model、もっと略してmodelと呼びます。\n",
    "まずは、modelの例を見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero_emb as dezero\n",
    "class DQNet(dezero.Models.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.l1 = dezero.L.Linear(128)\n",
    "    self.l2 = dezero.L.Linear(128)\n",
    "    self.l3 = dezero.L.Linear(1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = dezero.F.relu(self.l1(x))\n",
    "    x = dezero.F.relu(self.l2(x))\n",
    "    x = self.l3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dezeroは、ゼロから作るDeep Learning 3 フレームワーク編で紹介されているニューラルネットワークのフレームワークです。著者も中で書いていますが、基本はchainerです。chainerの開発はすでに終了しています。dezero_embは、jupyterliteのpyodideで動かすために、まとめて、一部変更したものです。\n",
    "\n",
    "Modelクラスの親クラスはLayerクラスで、Layerクラスの親クラスはVariableクラスです。\n",
    "\n",
    "DQNet <- Model <- Layer <- Variable\n",
    "\n",
    "Variableは、tensorと呼ばれているものに近い。\n",
    "\n",
    "Variableは、Variable間のつながりを重視しており、順伝播(forward)と逆伝播(backward)を実装しています。そして、つながりをつけることで、自動的に順伝播だけでなく逆伝播がつながります。逆伝播が自動的につながることを自動微分とも言います。微分は、曲線の傾きであることから、傾きの英語であるgradient、略してgradが使われます。\n",
    "\n",
    "forward関数に、x = F.relu(self.l1(x))のように、書かれているのが、つながりを示すものです。このように書くだけで、自動的に逆伝播も定義されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Linear\n",
    "\n",
    "　L.Lenearは、Lenear層です。コードをdezero_embから出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(dezero.L.Layer):\n",
    "    def __init__(self, out_size, nobias=False, dtype=np.float32, in_size=None):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None:\n",
    "            self._init_W()\n",
    "\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')\n",
    "\n",
    "    def _init_W(self, xp=np):\n",
    "        I, O = self.in_size, self.out_size\n",
    "        W_data = xp.random.randn(I, O).astype(self.dtype) * np.sqrt(1 / I)\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            xp = cuda.get_array_module(x)\n",
    "            self._init_W(xp)\n",
    "\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = w * x + b\n",
    "\n",
    "行列計算になっています。入力ｘ　に、重みwを掛けて、バイアスbを足しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Relu　&　Sigmoid\n",
    "\n",
    "functionで実装していますが、Relu層とSigmoid層して実装されることの多いレイヤーです。活性化関数ともいわれます。\n",
    "Reluは、正の数しかとさないフィルターで、Sigmoidは、シグモイド曲線に合わせて０から１までの範囲で出力します。\n",
    "\n",
    "linear層は行列計算なので、数値の掛け算の和になるため、オーバーフローしないようにするために使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 conv2d & deconv2d\n",
    "\n",
    "畳み込みニューラルネットワークで有名なconv2dとdeconv2dです。dezeroはどちらもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(dezero.L.Layer):\n",
    "    def __init__(self, out_channels, kernel_size, stride=1,\n",
    "                pad=0, nobias=False, dtype=np.float32, in_channels=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if in_channels is not None:\n",
    "            self._init_W()\n",
    "\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_channels, dtype=dtype), name='b')\n",
    "\n",
    "    def _init_W(self, xp=np):\n",
    "        C, OC = self.in_channels, self.out_channels\n",
    "        KH, KW = utils.pair(self.kernel_size)\n",
    "        scale = np.sqrt(1 / (C * KH * KW))\n",
    "        W_data = xp.random.randn(OC, C, KH, KW).astype(self.dtype) * scale\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.W.data is None:\n",
    "            self.in_channels = x.shape[1]\n",
    "            xp = cuda.get_array_module(x)\n",
    "            self._init_W(xp)\n",
    "\n",
    "        y = F.conv2d(x, self.W, self.b, self.stride, self.pad)\n",
    "        return y\n",
    "\n",
    "class Deconv2d(dezero.L.Layer):\n",
    "    def __init__(self, out_channels, kernel_size, stride=1,\n",
    "                pad=0, nobias=False, dtype=np.float32, in_channels=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if in_channels is not None:\n",
    "            self._init_W()\n",
    "\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_channels, dtype=dtype), name='b')\n",
    "\n",
    "    def _init_W(self, xp=np):\n",
    "        C, OC = self.in_channels, self.out_channels\n",
    "        KH, KW = utils.pair(self.kernel_size)\n",
    "        scale = np.sqrt(1 / (C * KH * KW))\n",
    "        W_data = xp.random.randn(C, OC, KH, KW).astype(self.dtype) * scale\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.W.data is None:\n",
    "            self.in_channels = x.shape[1]\n",
    "            xp = cuda.get_array_module(x)\n",
    "            self._init_W(xp)\n",
    "\n",
    "        y = F.deconv2d(x, self.W, self.b, self.stride, self.pad)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6　モデルの保存と読み込み\n",
    "　\n",
    "　dezeroのモデルは、モデルを保存するsave_weightsと読み込むload_weightsを持っています。どちらも中身は、numpyの保存と読み込みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 loss\n",
    "\n",
    "学習をするときに、モデルが出した推測値と学習用の期待値を比較します。例えば、モデルは70点と出したけれど、80点と出してほしかったとします。そこで、モデルに80点を出すように変更を加えるのですが、その差である10点に着目します。\n",
    "この10点を損失lossとよびます。\n",
    "\n",
    "　この10点分だけ補正をモデルにかけるべきでしょうか？\n",
    "\n",
    "そもそも、モデルは「だいたい正解」であればいいので、どんな状態の入力であっても、「だいたい正解」に近い推測値が出てくれればいい。それじゃあ、「だいたい正解」をどうすればわかるのでしょうか？\n",
    "\n",
    "そこで、ある程度の数をまとめて処理して、「だいたい正解」を出します。線形回帰あるいは、平均2乗誤差mean_squeared_errorと呼ばれています。ある程度の数は、バッチ数と呼ばれます。\n",
    "\n",
    "最適な学習用数値を出すために、backwardを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(dezero.Function):\n",
    "    def forward(self, x0, x1):\n",
    "        diff = x0 - x1\n",
    "        y = (diff ** 2).sum() / len(diff)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x0, x1 = self.inputs\n",
    "        diff = x0 - x1\n",
    "        gx0 = gy * diff * (2. / len(diff))\n",
    "        gx1 = -gx0\n",
    "        return gx0, gx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 optimizer\n",
    "\n",
    "lossで計算された、最適な学習用数値は逆伝搬されて、各モデル内の層に差分として残ります。この差分からウエイト、バイアスの数値を変更するのが、optimizerです。optimizerの違いが、学習効率にどの程度の影響があるかは、よく分かりません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9　詳細は\n",
    "\n",
    "dezeroの詳細は、ゼロから作るDeep　Learning３フレームワーク編を読んでください。本にも書かれていますが、自分で、ゼロから打ち込みながら勉強すると理解は進みます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
