{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9 Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 DQNには、replay　bufferが必要です。\n",
    "\n",
    "　DQNという機械学習の特徴というか、方法のど真ん中を理解して、replay　bufferを準備します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Q学習\n",
    "\n",
    "Deep Q Network　の　Q　は何でしょうか？\n",
    "\n",
    "どんな状態stateのときに、どんな行動actionをとるべきか？　これを方策といいます。ある行動を取ったら、報酬rewardが生じて、次の状態になります。この時に潜在価値のようなものをQとするならば、state0の時のQ(state0)と、行動をとった後の状態state1の時のQ(state1)は下のような関係になります。\n",
    "\n",
    "Q(state0) = reward + Q(state1)\n",
    "\n",
    "このQを行動価値関数とよびます。\n",
    "\n",
    "この関係は、actionの種類だけ存在します。actionが変われば、その結果も報酬も変わるのは当たり前ですね。\n",
    "\n",
    "このように、行動により次の結果が変わることをマルコフ過程と言います。　とるべき行動で次なる状態が多種多様になるときに、どのようにQは決まっていくのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 だいたいで考えてみる。\n",
    "\n",
    "　先のQの定義は、あるactionをした場合の価値なので、Qは多くのactionの集合体であると考えることができます。すると、actionが行われるであろう確率が分かれば、期待できる未来の潜在価値も予測できそうです。\n",
    "\n",
    "　ところで、厳密な確率などわかるのでしょうか？　現実社会でも、ダメだと分かっていることには手を出さないように、確率などを算出することは無意味に見えます。でも、いつも同じ方法を選んでばかりいては、違うもっといい方法を知ることもできません。ランダムにやり方を変えるべきことと、潜在価値の高い方法を選ぶことのバランスが必要です。\n",
    "\n",
    "　この「だいたい正しい答え」を探るのが、潜在価値を推測するコツになります。大切な部分です。\n",
    "\n",
    "だいたい正しい答え　＝　（　ランダムな方法　：　潜在価値の高い方法　）　比率を調整\n",
    "\n",
    "比率を調整する部分は、2つに分けられます。行動を起こす部分での調整と、学習する過程での調整です。行動を起こす部分はランダムに選択することをモンテカルロ法とも呼んでいます。そして、比率をイプシロンepsilonで表します。学習する過程では、replay_bufferを使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 replay buffer\n",
    "buffer_sizeの大きさの容器dequeにデータを蓄積していきます。超えると古い順になくなります。そして、batch_sizeをランダムに取り出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, buffer_size, batch_size):\n",
    "    self.buffer = deque(maxlen=buffer_size)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    data = (state, action, reward, next_state, done)\n",
    "    self.buffer.append(data)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.buffer)\n",
    "\n",
    "  def get_batch(self):\n",
    "    data = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "    state = np.stack([x[0] for x in data])\n",
    "    action = np.array([x[1] for x in data])\n",
    "    reward = np.array([x[2] for x in data])\n",
    "    next_state = np.stack([x[3] for x in data])\n",
    "    done = np.array([x[4] for x in data]).astype(np.int32)\n",
    "    return state, action, reward, next_state, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 replay bufferの役割\n",
    "ランダムな方法とと潜在価値の高い方法の比率を、どのように変えていくかです。最初は、ほとんどランダムに行動します。推測値に価値がないからです。\n",
    "\n",
    "学習が進むと、潜在価値が出てくるので、潜在価値に従って行動選択する比率を増やします。価値の高そうな方向に深堀していくイメージです。\n",
    "\n",
    "潜在価値の高い行動が増えると、その学習をした方が効率がよくなります。ランダムに動いたムダな行動結果を学習する意味はほとんどありません。そこで、ある程度のデータが蓄積されてオーバーフローすると自動的に古いものから捨てると便利です。\n",
    "\n",
    "この学習が進んで、深堀した時のデータを有効に使う仕組みこそが、replay bufferの役割です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 replay bufferのようなものは、他にはあるの？\n",
    "　画像データの学習などでは、画像データを取り出すときに、ブロックに分割する、k分割交差検証があります。dataloaderと呼ばれるランダムにバッチファイルを取り出す方法もあります。この２つを組み合わせるのが王道です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
